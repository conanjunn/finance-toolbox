# 计量金融


# 统计基础

[我的分享PPT](/attached/stat.pdf)

统计就仨大问题：
- 抽样分布：

你去对一个集合抽样，出样出来的样本是符合一定分布的，抽样分布就是研究这些分布到底是啥、有啥特性。
你从总体中抽取样本构造一个叫集合叫“统计量”，然后通过这些样本去推断总体性质。
讲人话就是：要用抽样来估算总体。

- 参数估计：

上面说，你研究抽样分布，最终目的也是为了研究总体分布。那么就引出参数估计，估计啥？估计总体分布。
总体的分布如果知道，那就是估计他的参数了；总体的分布要是不知道，那你只能估点泛泛指标（均值、方差）了。

- 假设检验


## 基础概念

你抽样出来得到一个集合（$$X_1,X_2,...X_n$$），然后你尝试用这些样本去推断整体的统计特性（比如分布、均值、方差啥的。）

这个抽样集合（$$X_1,X_2,...X_n$$），它也是有个分布的呀，它也是一组随机变量的值啊，它也是被看做是一个随机变量（有自己的分布啥的）

然后你用这个集合（$$X_1,X_2,...X_n$$）构建出一些新的**“函数”**出来，得到的又是个新的随机变量，这玩意给了个新名字：**“统计量”**，是的，统计量就是个**“随机变量”**，所以丫也有概率分布呀。统计量在统计之地位，相当于，随机变量在概率中的地位。

啥意思？我举个例子吧，加入你从全国孩子里抽样孩子们的身高值，抽出1000个孩子，随机的，那，这每个孩子就对应上面说的$$X_1,X_2,...,X_{1000}$$，这是一次抽样，然后，你可以放回这些孩子，再抽一次，你又得到一组1000个孩子的抽样；然后，你干了一件事，对这两次的抽样，求了一个平均值，这个平均值$$\bar{X}=\sum_1^1000 X_i$$，就是一个所谓的统计量，对，他就是一个随机变量，它有了2个值（第一次抽样的均值、第二次抽样的均值），然后，这个随机变量，也就个统计量，他也会有有个概率分布，恩，我们就是要研究它的特性。

充分统计量：就是你这些抽样出来的统计量，完全可以代表样本总体，不损失什么信息。

常见的统计量：

- $$\bar{X}=\frac{1}{n}\sum_1^n X_i$$
- $$S^2=\frac{1}{n-1}\sum_1^n (X_i -\bar X)^2$$
....

我们后面不讨论一个抽样集合了，而都是讨论这些**统计量**。

### 均值统计量

如果是总体分布是正态的，那么样本均值这个统计量，它符合一个正态分布：

$$\bar{X} ~ N(\mu,\frac{\sigma^2}{n})$$

注意注意！这里的$$\sigma^2,\mu$$，都是总体样本的哈。

那你可能会问了，如果总体不是正态分布的咋办？幸运的是，通过中心极限定理的证明，得到结论，样本均值$$\bar{X}$$的分布还是，上面这个式子（均值为总体均值，方差为总体方差的的1/n），但是，前提是n得比较大才行，那多大算“比较大”？一般认为是抽样数超过30个。

**中心极限定理：**

均值为$$\mu$$，方差为$$\sigma^2$$的总体，抽样n个样本，n足够大的时候，样本均值$$\bar{X}$$的抽样分布，近似符合方差为$$\mu$$，方差为$$\sigma^2/n$$的正态分布。

注意！

这句话暗藏玄机，不是说你抽样的结果符合正态分布，而是你抽样的样本的均值，符合正太分布，讲人话，就是你抽n个，得到一个均值，然后你又去抽，又得到一个均值，前面说了，这个均值，就是统计量，就是个随机变量。现在说的是，这玩意，这个抽样的的均值，丫符合正态分布，懂了么？敲黑板！是“样本均值“近似正态，不是样本近似正态！所以才叫“中心”极限定理嘛。

另外，你观察这个正态分布的方差是$$\sigma^2/n$$，n是啥？n是抽样的个数，n越大，这个方差越小，也就是，你抽样出来的均值，他离总体样本均值的波动，越来越小。

### 方差统计量

样本方差$$S^2$$的统计量，比较复杂，书上说，它和总体分布有关。

如果总体分布是正态的，他的抽样样本的方差的分布为n-1的卡方分布。（啥是卡方分布，下面会讲）


## 几种分布

前面说过，抽样出来的集合（$$X_1,X_2,...X_n$$），也是有分布的，他们的分布，他们的**“函数”（统计量）**，也有一个分布了吧。

这里多说一句，你不是抽样么，你可以抽无数次啊（假如是放回抽样），我们说这个分布，指的是无数次的抽样的分布，这里的分布，就是你无数次抽样后，
这帮统计量的分布。

好，这帮货都会呈现啥分布呢？答案是常见的有“$$\mathcal{X}^2卡方分布,t分布，F分布$$”，参考：[1](https://www.cnblogs.com/think-and-do/p/6509239.html),[2](https://blog.csdn.net/anshuai_aw1/article/details/82735201)吧。

### 渐进分布

抽样集合符合一个分布，当抽样量大了以后，这个分布会趋向一个稳定分布，这个分布就叫这个统计量的渐进分布。

提这个概念干啥，是为了引出下面的这玩意：

如果$$X_1,X_2,...$$是总体样本N（0,$$\sigma^2$$）的样本集，可以证明，
- $$\sqrt{n}\bar{X}/\sigma$$ ~ N（0，1）

这个结论很有用，对后面假设检验，就构建这个统计量，用于假设检验。


### $$\mathcal{X}^2$$卡方分布

当 $$X_i$$ 服从标准正态分布（N（0,1））,由它构造出来的统计量，$$\sum_1^n X_i^2$$ 服从$$\mathcal{X}^2$$卡方分布。

![](/images/20210615/1623748500317.jpg)

n叫做自由度，自由度可以解释为自由的独立变量的个数，我也没深究。

对于卡方分布，**一堆正态分布的平方和** 就呈现了卡方分布。

### T分布

X ~ N（0，1）标准正态分布，Y ~ $$\mathcal{X}^2$$卡方分布，那，他们的合体，$$t=\frac{X}{\sqrt{Y/n}}$$ 服从自由度为n的T分布。

![](/images/20210615/1623749125965.jpg)

好了，你现在可以用这玩意了，

你抽出来的样本（$$X_1,X_2,...X_n$$），我们用他们构建一个新的统计量，

$$\bar{X}=\frac{1}{n}\sum_1^n X_i, S^2=\frac{1}{n-1}\sum_1^n (X_i - \bar{X})^2$$，

你看，前者是个正态分布，后者是个$$\mathcal{X}^2$$卡方分布，两者一凑（当然还得做一下归一化成标准正态分布啥的），就合体完正好服从T分布了。

嘿！你看，理论结合实践了，用起来了，居然把凑出的统计量，套到了一个T分布上了，哈哈。这T分布用处可以多了，后面会用的。


$$\frac{\sqrt{n}(\bar{X}-\mu)}{S} ~ t(n-1)$$

总结一下，**一个正态分布除以一个卡方分布，就变成了T分布**。

### F分布

再接再来，我们再“凑”一个新的分布出来！

先说说新引出的F分布，

Y 服从~ 自由度为m的$$\mathcal{X}^2$$卡方分布，Z 服从~ 自由度为n的$$\mathcal{X}^2$$卡方分布，俩合体一下，就变成了F分布。

$$X=\frac{Y/m}{Z/n}=\frac{nY}{mZ}$$

这合体，称作第一自由度为m，第二自由度为n的F分布：F（m,n）

![](/images/20210615/1623749731311.jpg)

总结一下，**一个卡方分布分布除以另外一卡方分布T分布，就变成了F分布**

## 参数估计

前面讨论了，用抽样样本，可以构建出“统计量”，用统计量，是不是可以反向估计出整个总体的特性呢？要是能，该多好啊！

我先斗胆、朴素、naive地估计一下：用样本均值，去估计总体的均值？用样本的方差去估计总体的方差？成不？

成！

### 点估计

这种方法，就叫做**“点估计”**。

虽然，理论上，你抽样的均值的期望等于总体的均值，讲人话，就是，你抽无数次，抽出个无数次的样本的均值，这些均值，最终的期望是真实总体的均值：

$$E(\bar{X})=\mu$$

但是，你不可能抽无数次，你的随机和偶然性，必然导致你用这些个抽样均值，去代表总体均值，是不准确的。

那么问题来了，“不准确的”，这种话就是屁话，有多不准确？你能给我一个量化的数么，概率也行啊。

答案是，不行！点估计给不出，所以，要引出区间估计。

### 区间估计

与点估计不同，点估计出一个值，区间估计给出一个**范围**。且，给出总体的值（比如总体的均值），落在这个区间的概率。

还记得，前面的**中心极限定理**不？

>均值为$$\mu$$，方差为$$\sigma^2$$的总体，抽样n个样本，n足够大的时候，样本均值$$\bar{X}$$的抽样分布，近似符合方差为$$\mu$$，方差为$$\sigma^2/n$$的正态分布。

我噻！看到正态分布了！为何我这么激动，有一个可以确定的分布了啊，我就能干好多事儿了啊。

来！我们来看看这个正态分布：我问你，这个正态分布的随机变量是是谁？是抽样的均值！别晕！
你抽一次（抽出100个样本），你算个均值出来，这个均值，就是一个随机变量的值；然后你再抽100个，又得到一个；你再抽、再抽、...
然后，你落在3个方差之外的概率是95%啊，我勒个去，给出了“落在这个区间的概率”了（就是前面提出的问题）。

这里千万别晕，你其实是不知道总体的$$\mu,\sigma$$的，对，你不知道；你知道只有一次次抽样的均值，我管它们叫:抽样均值1$$\bar{X}\_1$$,抽样均值1$$\bar{X}\_1$$,抽样均值1$$\bar{X}\_1$$,...，它们是一组随机变量的值，对吧？

接下里的描述，太精妙了，我不自己组织语言了，我引用书上的话吧：

我们可以求出样本均值$$\bar{x}$$落在总体均值$$\mu$$的两侧的概率，但是，实际估计时候，我们只能知道$$\bar{x}$$，我们反倒是不知道$$\mu$$，它恰好是我们要求的东东。
**如果样本的均值$$\bar{x}$$落在总体均值$$\mu$$的两个总体标准差$$\sigma$$内，那么你也可以说成是，总体样本的均值$$\mu$$是落在样本均值$$\bar{x}$$的两个总体标准差$$\sigma$$之内**。
比如，用100次抽样出来的100个均值，有95个的样本均值+2个总体标准差$$\sigma$$范围内，包含了总体样本的均值$$\mu$$。

大白话说出这事，就是，哥们我抽样了1次，抽了N个（这个数不重要），得到一个均值$$\bar{X}$$，我现在就可以拍着胸脯说，兄弟，你这个均值$$\bar{X}$$，你左右探出去总体方差$$\sigma$$（但是，你不知道啊这个$$\sigma$$哈，强调一下），这个区间内，总体样本的均值$$\mu$$落在里面的概率是95%。

这里，我们一直都不知道总体样本的均值和方差$$\mu,\sigma$$啊，再次强调一遍，别晕，我们就是要求他们俩，但是，我们为了求他们俩，我们去抽了100次的样，得到了100个样本均值$$\bar{X}$$，我们只能用这些均值，来反向告诉别人，总体样本均值$$\mu$$，大概是落在这里面，多大概呢？那就得知道总体样本的方差$$\mu$$啊，可是这玩意，我不知道啊。不过，我看书上说，如果知道，就可以直接套用（我就奇了个怪了，您总体均值都待定呢，咋个就知道方差了呢）；如果不知道，就可以用抽样样本的方差来近似代替，别急，前提是样本数量n要足够多，一般要求n>30。

再说一下这个置信区间怎么求呢？

你可能说，用我估计的均值$$\bar{X}$$，加上2个标准差，不就是了么？

这里有几个问题：

1、为何加2个标准差，不加1个，或者3个？

答：这个需要反着想，你给出一个所谓的$$\alpha$$，$$\alpha$$没有中文对应名字，但是$$1 - \alpha$$有，叫**置信水平**，啥叫置信水平，就是个概率值，是你这个样本均值$$\bar{X}$$中包含真实的总体样本均值$$\mu$$的概率。如果这个是一个正态分布，可以反向求出对应的概率的对应的x值，
这个值叫做“$$z_{\alpha/2}$$”。

多说一句，这个$$z_{\alpha/2}$$，实际上是标准正态分布$$N(0 \sim 1)$$对应的x，为何是标准正态分布呢？

$$
\begin{aligned}
	\bar{x}  \sim  N(\mu, \frac{\sigma^2}{n}) \\
	\frac{\bar{x} - \mu}{\sigma^2 / n}  \sim  N(0,1) \\
\end{aligned}
$$
对正态分布做变换成为标准正态分布，目的为了方便查表求$$z$$。

所以，给定了置信水平对应的$$\alpha$$，就可以查表求出$$z$$，然后就可以得到对应的置信区间了$$\bar{x} \pm z \frac{\sigma}{n}$$。

吐槽一下，为何叫z呢，因为，对应的是标准正态分布，就是用z来表示的，哈哈。


2、这个标准差是啥？是样本的标准差么？还是总体的标准差？如果总体的标准差不知道呢？

这个标准差，是总体的标准差，如果总体样本的$$\sigma$$未知，可以用抽样数大于30的样本方差$$S^2$$近似代替。

3、说我的置信区间包含真值的概率是$$1-\alpha$$，这个说法对不对？

不对！书上专门强调了这点。很多时候，我们只做一次抽样，得到一个确定的置信区间（给定置信水平\alpha的要求后），总体均值，也是确定的，这个时候，置信区间和总体均值$$\mu$$，之间就是包含、不包含，二选一。他不是个概率问题！但是，如果你抽样100次，给定同样的置信水平$$\alpha$$的情况下，就得到$$100 \* (1-\alpha)$$次包含总体均值$$\mu$$的情况。所以，需要这样理解才精确。

不过，我觉得，你认为置信水平就是“置信区间包含真值的概率”，也没啥大问题，无关大雅，个人的感觉。

![](/images/20211026/1635242116714.jpg)

做了20次抽样，得到20个置信区间（每次给定一个置信水平0.95），有一次“不幸”漏掉了总体的均值$$\mu$$。

### 评价估计量

说道估计参数，比如总体的均值，你用的是样本的均值这个统计量，那你为何不用抽样样本的中位数呢？可以啊，你完全可以用，但是，效果肯定不如抽样的均值好。

这就涉及到一个话题，你如何评价一个统计量，是不是一个更好、最好的统计量，这里有3个原则，来评价他们：

- 无偏性

这个意思就是说$$E(\hat{\theta})=\theta$$，$$\theta$$是总体的参数，$$\hat{\theta}$$是估计量，这意思是说，你用来估计的统计量，他们的均值，应该等于真实的你要估计的总体的那个参数，这就叫**无偏性**。“我估的和真实的总体参数一样”。

- 有效性

另外，你的统计量，和我的统计量，咱俩都具备无偏性，但是，我的方差小，你的方差大，那我的就比你的好，也就是说，估计量（统计量）的方差更小的，更有效，这个特点叫**有效性**，“我比你有效”。

- 一致性

假设某个统计量（估计量）是个无偏估计，你用100个抽样的均值去估计真实总体均值，得到一个方差；然后你用1000个继续估计，应该到的方差更小。n越大，和真实总体的参数的方差应该越小，这个特性就叫**一致性**。“n越大估的越准”。


### 抽样数量

一个很重要，但是被前面一直忽略的问题，就是，需要抽样多少个样本，才能比较好的估计出来总体呢？



### 总结

总结一下，用样本均值直接当做总体均值，这方法叫点估计，不好。

用区间估计，可以给出某个置信度（$$1-\alpha$$），也即是相信总体的均值$$\mu$$（是个确定值）落在某次抽样均值$$\bar{x} \pm \z_{\alpha/2} \frac{\sigma}{n}$$ 区间概率是置信度（$$1-\alpha$$）。

![](/images/20211026/1635241442549.jpg)

## 假设检验

啥是假设检验？

参数估计，是用样本去估总体参数，而假设检验，是先假设一个参数情况，然后用样本去验证它，是否靠谱。

举个例子：我假设北京市学生高一男生身高是$$173 \pm 3$$，然后，你去各学校抽样，然后假设检验，你就可以接受这个参数正确的假设，或者推翻这个参数估计正确的假设。

开始这个假设，叫**原假设$$H_0$$/零假设**，它的相反的假设叫**备择假设$$H_1$$**，他们俩互斥，否定一个，意味着接受另外一个。

### 原理

这个假设检验怎么来的呢？怎么就想出这么一个办法，来检验你估计的参数靠谱不靠谱呢？

假设检验，是基于**小概率原理**，所谓小概率原理，是指，“*发生概率很小的事件，在一次实验中几乎是不可能发生的*”。

这里有个几个点：
- 小概率事件，这个概率有多小？一般是0.05，是英国概率学家费希尔给的建议值，大家都沿用了。
- 你假设了一个参数，然后，你用这个参数去算某一次事件的概率，如果这个概率小于0.05，那说明你的假设不靠谱啊，你的假设下，应该大概率发生才对；现在小概率发生了，说明你的假设不对啊。


### 步骤

假设检验的步骤：

1、提出原假设和备择假设，原假设就是你想验证的结论，备择假设是它的方面。

2、指定检验中的显著性水平，就是他们一般说的$$\alpha$$，往往都很小，比如0.05，$$1-\alpha$$就是**置信度**。

3、收集样本数据并计算检验统计量的值，就是算一下样本们的方差啊、均值啊

4、利用检验统计量的值计算p-值

5、p-值法的拒绝法则，如果p-值<=显著性水平的值，则拒绝原假设H0

6、解读统计结论

**【p值是什么，$$\alpha$$是什么？】**

[参]: [1](https://zhuanlan.zhihu.com/p/26068572)，[2](https://zhuanlan.zhihu.com/p/86178674)，[3](http://www.eastwin-med.com/newsitem/278412468)

其实就是概率，你假设检验了$$H_0$$,那$$H_0$$发生的概率就是p，要是这数。


### 正态检验

看一个序列是不是符合正态分布。话说，正态检验太重要了，如果一个时间序列不符合正态分布，那后面的检验都没有意义了。

常见的有SW检验（样本<50）和KS检验（样本>50），[示例代码](https://github.com/piginzoo/fund_analysis/blob/master/test/test_stat.py)。

#### SW:Shapiro-Wilk检验（小数据<50）


Shapiro-Wilk检验和Lilliefor检验都是进行大小排序后得到的，所以易受异常值的影响。

Shapiro-Wilk检验只适用于小样本场合（3≤n≤50）

$${\left(\sum_{i=1}^n a_i x_{(i)}\right)^2 \over \sum_{i=1}^n (x_i-\overline{x})^2}$$


[参]：[1](https://zhuanlan.zhihu.com/p/26539771),[2](https://www.jianshu.com/p/e202069489a6),[3](https://www.biaodianfu.com/python-normal-distribution-test.html),[4](https://zhuanlan.zhihu.com/p/26477641),[5](https://www.medsci.cn/article/show_article.do?id=0805180e5132)

#### KS:Kolmogorov-Smirnov检验（小数据>50）

柯尔莫戈洛夫-斯米诺夫检验（Kolmogorov-Smirnov test），简称K-S检验

K-S检验适合用于大数据样本的正态性检验，当样本的数据量超过50行时，它被认为是一个大样本，我们倾向于看K-S检验的分析结果得出结论。



### T检验

T检验，又叫做学生T检验，用于样本含量较小（如n<30），总体标准差$$\sigma$$未知的正态分布。

t检验是用t分布理论来推论差异发生的概率，从而**比较两个平均数的差异是否显著**。与f检验、卡方检验并列。

## 方差分析

方差分析干嘛用的？

方差分析，是为了看某个因素，是不是影响，另外一个因素。

>它是研究，**分类**型**自**变量，对**数值**型**因**变量的影响：有没有影响，影响有多大。

[教材](https://book.douban.com/subject/30422797/)上，给的例子是，4个行业，对投诉的次数的影响：
比如零售、旅游、航空、制造，这4个行业，每个行业里有若干公司，这些公司规模相当，然后每个公司有投诉次数。
看这个数据情况，希望得到一个判断：行业对投诉有无影响？

先说一下，涉及到的概念：
- 因素/因子：就是因变量，就是要被检验的东西，这里就是投诉数量
- 自变量：就是引起变化的源，也就是行业
- 水平：就是自变量里面的离散的值，这里就是这4个行业，4个值，还有，每个水平，都独立出来一个“总体”，这个例子里，4个行业，就相当于把总体化成了4堆，这4个要分别对待。


现在，就是要看，自变量（也叫因素、因子），是不是影响因变量？影响有多深？要干这事，工具就是方差分析。

方差分析有一些假设：
- 各个总体（就是各个行业自己的各自公司的投诉数）要符合正态分布
- 方差$$\sigma^2$$相同！这单很重要，就是叫“方差分析”的由来。

所以，自变量（行业）对因变量（投诉数）有没有影响，就转变成了，每个行业的投诉均值、方差的考察问题。
朴素上讲，如果各行业对投诉数没影响，那均值和方差应该大抵相同才对。
但更科学的方法是，用假设检验。

先提出假设：自变量对因变量没有影响，$$H_0: \mu_1 = \mu_2 = ...$$（即行业对投诉量没有影响）

那么，对应的备择假设$$H_1: \mu_1 ,\mu_2, ...$$，不全相等（即行业对投诉量有影响）

那，就需要构建统计量了，还得知道，统计量符合啥分布，最后，就可以通过算概率，来接受还是拒绝原假设了。

构建统计量：
- 算各个总体的均值（各个行业的投诉的均值）
- 算全部的均值（把各个行业都放到一起，算所有行业的投诉均值）
- 算误差平方和
	- 总体平方和SST：全部观察值和全部均值的误差平方和
	- 组内平方和SSE：又叫因素平方和，各组均值和全部均值的误差平方和
	- 组间平方和SSA：各个样本和组内均值的误差平方和
- 组间均方MSA：比较组内均方和组间均方的差异，$$MSA=\frac{SSA}{k-1}$$，k为因素水平个数，我们的例子是4个行业，4
- 组内均方MSE：$$MSE=\frac{SSE}{n-k}$$，n是全体（4个行业加一起）的样本数

经过上述准备，我们终于构建出一个大法器：

$$F=\frac{MSA}{MSE} \sim F(k-1,n-k)$$

这是一个F分布，然后用这个分布，就可以去根据你要求的置信水平$$\alpha$$，去做假设检验了，就不细赘了。

大体思路就是这样，我其实也没有再深入，感觉上暂时用不到，搞清楚大体思路，就可以了，用到的时候，再深入研究，别太浪费我的精力分配。

**总结一下吧，**

方差分析，他要解决的是，某个离散型因素，是不是影响某个结果，他们之间是不是有相关性？（多强的问题，我没有再深入研究，实际上，[教材](https://book.douban.com/subject/30422797/)也给出了，就是$$R^2$$分析），为了干这个是，做了一堆正态、方差假设，然后费了一番劲，构建了一个符合F分布的统计量，用它来做假设检验。
朴素的假设，就是你们这些不同的离散值对应的子总体，你们的均值应该差不多才对。通过这个假设检验，反向判断出相关性与否的结论。


## 一元回归

一元回归，主要是来用来探讨，数值因变量（$$y/f(x)$$）和数值自变量$$x$$之间，是不是有关系，有怎样的关系的一个数学工具。
这里的自变量$$x$$，只有一个，所以才叫一元回归，多元回归我们下一节讨论。


### 相关性分析

为了分析变量之间的关系，先提出一些灵魂拷问：
- 变量之间存在关系么？（如果没关系，就不能称之为因变量、自变量啥的了）
- 如果存在关系，是什么关系？（如何用数学刻画出来）
- 这种关系的强弱如何？
- 样本之间反映的关系能代表总体么？（这个其实很重要，毕竟你得到的数据都是抽样数据，你分析抽样数据有关系，就代表总体上也有这种关系么？）

**散点图**是个好东东，[教材](https://book.douban.com/subject/30422797/)给出了示例：

![](/images/20211027/1635305145769.jpg)

形象化体会后，我们给出数学上的度量方法：

**相关系数 correlation coefficient**

总体的相关系数记作$$\rho$$，总体的相关系数记作$$r$$。

$$r=\frac{n \sum x y-\sum x \sum y}{\sqrt{n \sum x^{2}-\left(\sum x\right)^{2}} \cdot \sqrt{n \sum y^{2}-\left(\sum y\right)^{2}}}$$

也可以表达成：

$$
r=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}
$$

这两种表达是等价的，参考[皮尔逊积矩相关系数](https://zh.wikipedia.org/wiki/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%A7%AF%E7%9F%A9%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0)。

这个公式也称作：线性相关系数、Pearson相关系数

- r的取值范围\[-1,1\]，0是无关，1是完全正相关，-1是完全负相关
- r只能表示x和y有**线性**关系，但是给不出具体的线性关系
- r不能反映非线性关系，极端的时候r=0，xy可能都有**非线性**关系

接下来一个问题，$$r$$可不是总体的相关性$$\rho$$，它只是抽样的，那么一个问题是，这个抽样能代表总体么？也就是所谓r的可靠性、显著性。

所以，我们先得琢磨琢磨，这个抽样的相关系数$$r$$，它是个统计量啦，它也是个随机变量啦，它到底符合什么分布？

[教材](https://book.douban.com/subject/30422797/)上没给出推导，只给了结论：
- 当总体是正态分布，随着n的增大，抽样的相关系数$$r \to$$ 正趋向于态分布。
- 但是，上述仅在总体的相关系数$$\rho=0$$时候更趋近，但是在$$\rho = \pm 1$$两头时候，会呈现偏态：$$\rho$$负值$$r$$值右偏，$$\rho$$正值时候$$r$$左偏

所以，不能用正态分布来估计$$r$$的分布了，那用啥？用t检验！（至于为何用t检验，教材上也没细说）

这样，假设检验的流程就变成：

- 提出$$H_0: \rho = 0 , H_1: \rho \ne 0$$
- 计算检验所使用的统计量：$$t = \|r\| \sqrt{\frac{n-2}{1-r^2}} \sim t(n-2)$$

*t分布咋来的来着？X ~ 标准正态分布，Y ~ 卡方分布，那，他们的合体，$$𝑡=\frac{𝑋}{\sqrt{𝑌/𝑛}}$$ 服从自由度为n的T分布；
而卡方分布，正是一堆正态分布的平方和* 这里的t分布咋推出来的？[教材](https://book.douban.com/subject/30422797/)上没给出推导，只给了结论。

然后，给定一个显著性水平$$\alpha$$，算出对应的t，看t的范围，得到是否接受原假设。

### 回归公式

前面的相关性，只给出了变量间的关系强度度量，但是，我们还需要一个明确的数量关系的描述，这个就需要回归分析了。

一元回归的表达式：$$ y = \beta_0 + \beta_1 x + \epsilon$$

对这个式子的理解：
- $$\beta_0 + \beta_1 x$$，是对线性部分的刻画
- 误差项$$\epsilon$$反应的是线性关系外的随机影响，不能被线性关系解释的变异性，它的期望$$E(\epsilon)=0$$，服从一个正态分布$$\epsilon \sim N(0,\sigma^2)$$
- 这个式子，是假定$$x,y$$是存在线性关系的
- 假定x都是确定值，而回归出来的y是个随机变量，它是$$\beta_0 + \beta_1 x$$加上一个$$\epsilon$$扰动项，但是$$y$$的期望$$E(y)=\beta_0 + \beta_1 x$$
- 误差项$$\epsilon$$彼此间不想关，就是$$x_1$$对应的$$\epsilon_1$$和$$x_2$$对应的$$\epsilon_2$$没有任何关系
- 上句话，也可以表达成，任何$$y$$都服从均值为$$\beta_0+\beta_1$$，方差为$$\sigma^2$$的正态分布

![](/images/20211027/1635316979454.jpg)

### 参数估计

一元回归的表达式中，$$\beta_0,\beta_1$$是未知的，需要我们用数据去估计他们，我们也没法用总体去估计他们，我们只能用抽样的值，去估计他们，
这也就是样本统计量 $$\hat{\beta_0},\hat{\beta_1}$$（我们始终是在用样本的统计量去估计总体，始终牢记这点），我们就得到了样本回归方程：

$$ \hat{\y} = \hat{\beta_0} + \hat{\beta_1} x$$

然后我们用最小二乘法，得到最优的样本数据对应的$$\hat{\beta_0},\hat{\beta_1}$$

$$\sum\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)^{2}$$

也就是求解这个方程的最小值，通过求极值，求偏导，可以得到一个解析解（不推导了，请参考[教材](https://book.douban.com/subject/30422797/)）：

$$
\left\{\begin{array}{l}
\hat{\beta}_{1}=\frac{n \sum_{i=1}^{n} x_{i} y_{i}-\sum_{i=1}^{n} x_{i} \sum_{i=1}^{n} y_{i}}{n \sum_{i=1}^{n} x_{i}^{2}-\left(\sum_{i=1}^{n} x_{i}\right)^{2}} \\
\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{array}\right.
$$

当然，回归也不用你这么费劲去算，用Excel的“回归统计”功能，或者各种开发语言中的软件包，可以方便求解。

### 拟合度评价

虽然我们可以拟合出直线，但是，这个直线到底好不好，是不是“完美”或者“很垃圾”地拟合了抽样数据和变量呢？
我们使用给一个叫**判定系数$$R^2$$**的概念:

$$R^{2}=\frac{S S R}{S S T}=\frac{\sum\left(\hat{y}_{i}-\bar{y}\right)^{2}}{\sum\left(y_{i}-\bar{y}\right)^{2}}=1-\frac{\sum\left(y_{i}-\hat{y}_{i}\right)^{2}}{\sum\left(y_{i}-\bar{y}\right)^{2}}$$

这个式子里SSR、SST啥意思？以及各种表示、意义都必要好好说一下：

先说表示：
- $$\hat{y}_i$$，就是直线拟合出来的y，就是直线上的点的y
- $$\bar{y}$$，是所有样本真实y的平均值
- $$y_i$$，是某个样本的真实y值

![](/images/20211027/1635319049772.jpg)

接下里解释含义，先推导：

$$
\begin{aligned}

\sum\left(y_{i}-\bar{y}\right)^{2} &=\sum\left(y_{i}-\hat{y}_{i}\right)^{2}+\sum\left(\hat{y}_{i}-\bar{y}\right)^{2}+\\
	& 2 \sum\left(y_{i}-\hat{y}_{i}\right)\left(\hat{y}_{i}-\bar{y}\right) \\
	\because \sum\left(y_{i}-\hat{y}_{i}\right)\left(\hat{y}_{i}-\bar{y}\right) & = 0 \\
	\therefore 
	\sum\left(y_{i}-\bar{y}\right)^{2} &=\sum\left(y_{i}-\hat{y}_{i}\right)^{2}+\sum\left(\hat{y}_{i}-\bar{y}\right)^{2}
\end{aligned}
$$

我们给出上式中各项命名：

总平方和SST  : $$\sum\left(y_{i}-\bar{y}\right)^{2}$$
残差平方和SSE: $$\sum\left(y_{i}-\hat{y}\_{i}\right)^{2}$$

回归平方和SSR: $$\sum\left(\hat{y}\_{i}-\bar{y}\right)^{2}$$

上面的推导结果，就可以表达成： 总平方和SST = 回归平方和SST + 残差平方和SSE

这个式子表达了什么含义呢？

我换个表达：

一个真实y和真实y们的均值的差的平方SST，可以分解成 = 拟合出来的y和真实y们的均值的平方差（这个是说，你用直线拟合出来的部分/值）SSR + 剩下的你拟合不出来的残差们和真实y们的均值的平方差SSE

大白话就是，

总的和基准（真实y的均值）的差距SST = 可以模拟出来的差距SSR + 模拟不出来的差距SSE

然后，我们再看判定系数$$R^2$$，就迎刃而解了：

判定系数$$R^{2}=\frac{SSR}{SST}=\frac{\sum\left(\hat{y}_{i}-\bar{y}\right)^{2}}{\sum\left(y_{i}-\bar{y}\right)^{2}}$$

那大白话，就是，可以模拟出来的差距 除以 总差距，**判定系数$$R^2$$越大（约接近1），说明拟合的越好**呗！

这里有个结论，很诡异：

**相关系数$$r$$是判定系数$$R^2$$的平方根**，这俩貌似没啥关系的东西，居然是这么一个关系。相关系数$$r$$是看x和y的相关性的，$$R^2$$是度量回归方程拟合程度的，
这俩居然可以相互照应起来。线性回归拟合越好，说明相关性越高，噢！符合直觉啊，赞了。

至于推导，懒得写了，参考这篇把[相关系数和R方的关系是什么？](https://www.zhihu.com/question/32021302)。

### 显著性检验

你做了一元回归，人家到底是不是线性关系啊？你是不是应该做假设检验？

除此之外，你还得保证$$\hat{\beta}\_1$$这个和$$x$$相乘的系数，也得不为0啊，否则，这就是一条直线了啊。这个也需要假设检验（检验不为零）。

#### 线性回归检验!

检验线性，是通过一个比较复杂的统计量来检验的：$$F=\frac{SSR/1}{SSE/(n-2)}=\frac{MSR}{MSE} \~ F(1,n-2)$$。

其中：
- SSR: 回归平方和，$$\sum\left(\hat{y}_{i}-\bar{y}\right)^{2}$$，就是拟合值$$\hat{y}$$和均值$$\bar{y}$$的差异，是体现线性关系表达的能力
- SSE: 残差平方，$$\sum\left(y_{i}-\hat{y}_{i}\right)^{2}$$，他体现你预测的，和真实值之间的差，是线性表达不了的差异。
- MSR: SSR除以自由度（一元回归是1，就是参数数量-1）
- MSE: SSE除以相应自由度（SSE自由度是$$n-k-1$$）,一元回归k=1，所以是n-2。
- 最后，构建出的统计量$$\frac{\sum\left(\hat{y}_{i} - \bar{y}\right)^{2}/1}{\sum\left(y\_{i}-\hat{y}_{i}\right)^{2}/n-2}=\frac{SSR/1}{SSE/(n-2)}=\frac{MSR}{MSE}$$，它恰好符合F分布（F分布啥来着来着？是nY/mZ，Y和Z都是$$\chi$$卡方分布，而卡方分布又是正态分布的平方和的分布；观察这个式子，F分布可能就是如此得出的，我猜）。

终于，我们可以用上述的构建的统计量来做检验了：
 
1、提出原假设$$H_0:\beta_1=0$$，也就是线性关系**不**显著

2、计算统计量$$F=\frac{SSR/1}{SSE/(n-2)}=\frac{MSR}{MSE} \~ F(1,n-2)$$

3、给出一个显著水平$$\alpha$$，确定自由度，一个是1，一个是n-2，查表可得临界值$$F_\alpha$$，如果$$F>F_\alpha$$，拒绝原假设$$H_0$$，即线性相关。否则，线性相关不明显，或者说不相关。


#### 回归系数检验

上面是判断线性关系，接下来似乎判断系数$$\beta_1$$不为0，原因是，它为0，线性相关就没有意义了。

我靠！你发现没有，这怎么和上一个线性相关的检验的标准一样啊，都是$$\beta_1\ne 0$$啊，其实，是有区别的。区别在于，检验用的统计量不同。这里确实有些诡异，不过，上面那个跟强调检验线性关系，而下面这个重点在于系数不为0，不过我说的自己都心虚，先姑且这样理解吧。

自问自答：我的猜想是对的！上面的F检验和下面的t检验，是一样的，是等价的，当然，这个只对一元回归，多元回归就不行了，原因是多元回归有多个参数了，得一个一个地检验了。

这个统计量为：$$t=\frac{\hat{\beta_1} - \beta_1}{s_{\hat{\beta_1}}}$$

这个式子怎么来的呢？

首先，教材上说，$$\hat{\beta}_1$$服从正态分布，标准差为$$\sigma_{\hat{\beta1}_1} = \frac{\sigma}{\sqrt{\sum x^2_i - \frac{1}{n}(\sum x_i)^2}}$$，$$\sigma$$是误差项$$\epsilon$$的标准差，这个不得而知，所有用它对应的估计量$$s_e$$近似替代，得到估计量的$$\hat{\beta}_1$$的方差$$s_{\hat{\beta}\_1}=\frac{s_e}{\sqrt{\sum x^2_i - \frac{1}{n}(\sum x_i)^2}}$$。
 
其中，$$s_e$$为抽样的预测$$\hat{y}\_i$$和真实值$$y_i$$的误差项$$\epsilon$$的标准差估计。这里需要解释一下，你用样本估了一条直线，你就得到了一条连续曲线，这条连续上所有的y和总体，总是可以算出所有的误差$$\epsilon$$的，但是，由于你没法知道这个值，你还得用你的那些样本算一个这些样本对应的$$\hat{\epsion}$$，这个值其实就是$$s_e$$。

$$
s_{e}=\sqrt{\frac{\sum\left(y_{i}-\hat{y}\_{i}\right)^{2}}{n-2}}=\sqrt{\frac{S S E}{n-2}}=\sqrt{MSE}
$$
 
而这个统计量：$$\frac{\hat{\beta_1}-\beta_1}{s_{\hat{\beta_1}}}$$，符合自由度为2的t分布。

这样，就可以使用这个统计量做假设检验：

1、提出原假设$$H_0:\beta_1=0$$，也就是线性关系**不**显著

2、计算统计量$$t=\frac{\hat{\beta_1}-\beta_1}{s_{\hat{\beta_1}}}$$

3、给出一个显著水平$$\alpha$$，自由度是n-2，查表可得临界值$$t_{\alpha/2}$$，如果$$t>t_{\alpha/2}$$，拒绝原假设$$H_0$$，$$\beta_1 \ne 0$$。否则，接受原假设$$\beta_1=0$$。

### 利用回归方程预测

TODO

### 残差分析

在回归模型$$y=\beta_0 + \beta_1 x + \epsilon$$中，对$$\epsilon$$的要求是均值为0，正态分布，方差相等，所以，同归对残差$$\epsilon$$的检验，可以反向验证你的回归模型正确与否。

所以，可以将残差标准化后，对其进行正态检验。

$$z_{e_i} = \frac{e_i}{s_e} = \frac{y_i - \hat{y}\_i}{s_e}$$

- $$s_e$$是残差的标准差估计。

## 多元回归

多元回归，是指，因变量是和多个自变量之间存在线性关系。

$$y=\beta_0 + \beta_1 x_1 +  \beta_2 x_2 + ... + \beta_k k + \epsilon$$

$$\epsilon$$是个随机变量，他的期望为0，$$E(\epsilon) = 0$$，方差也是固定的。另外，不同x的输入，得到的$$\epsilon$$，彼此是不相关的，你的和我的，得到的$$\epsilon$$，彼此无关。

### 拟合度评价

跟一元回归一样，我们仍需要判断我们的线性模型拟合的好坏，同样，我们还是用$$R^2$$来作为我们评价拟合程度的指标（这里细节可以翻到一元回归里复习），不过，多元回归中，有个问题：

>当增加变量后，会使预测误差变小，这样残差平方和SSE就变小，SSR=SST-SSE，SST恒定时候，SSR就变大，从而使得$$R^2$$变大，从而高估$$R^2$$。

[教材](https://book.douban.com/subject/30422797/)*上这样说的，不是特别理解，我的理解是，多个变量来解释y，可能会过拟合，导致$$R^2$$变大*。

解决的办法，是使用**调整的多重判定系数（Adjusted Multiple Coefficient of Determination）**：

$$R^2_a = 1 - (1-R^2)\left(\frac{n-1}{n-k-1}\right)$$

这个是考虑了样本量后，做出了调整后的$$R^2$$，更客观。

### 显著性检验

之前，还记得一元回归中，你“线性关系检验-是不是线性”，以及你“系数检验-系数不能为0”，是等价的、一样的，都是验证$$\beta_1 \ne 0$$，只是统计量不同，但是是等价的。

但是，在多元回归中，线性关系检验，和，系数检验就不等价了，原因很简单，系数现在有个多个了，得一个一个地检验了。

比如我有5个系数，其实只有2个是独立的，另外3个是是共线的（就是彼此狠雷同），这样，线性检验是可以通过的，但是，系数检验，就会发现那2个是通过检验，剩下的3个是无法通过检验的。这个时候，就需要出现这些共线系数。

#### 线性关系检验


1、线性关系的检验，转化成这个$$H_0$$假设：

$$H_0: \beta_1 = \beta_2 = ... = \beta_k = 0$$

$$H_1: \beta_1, \beta_2 , ... , \beta_k$$，至少有一个不为0

如果$$H_0$$成立，就不是线性，否则，就是线性。

2、然后，我们构建统计量：

$$F=\frac{SSR/k}{SSE/(n-k-1))} ~ F(k,n-k-1)$$，然后我我们根据我们真实的样本算出这个F值。

这个和一元回归是类似的，只不过那个时候，k=1而已。

3、然后我们给出一个显著水平$$\alpha$$，根据自由度k，去查表达到对应的$$F_{\alpha}$$值。

如果$$F>F_{\alpha}$$，拒绝原假设$$H_0$$，接受备择假设$$H_1$$，即，符合线性关系。否则，不拒绝原假设$$H_0$$，即不符合线性关系。


#### 系数检验

和一元回归类似，对系数检验采用t检验，但是，不同于一元回归，多元回归有多个系数，所以要逐一地进行检验，方法和一元回归一模一样：

统计量为：$$t=\frac{\hat{\beta_i}-\beta_i}{s_{\hat{\beta_i}}},i>0$$，它符合自由度为n-k-1的t分布。

其中，$$s_{\hat{\beta}\_1}=\frac{s_e}{\sqrt{\sum x^2_i - \frac{1}{n}(\sum x_i)^2}}$$。

这样，就可以使用这个统计量做假设检验：

1、提出原假设$$H_0:\beta_i = 0, H_1:\beta_i \ne 0$$

2、计算统计量$$t=\frac{\hat{\beta_1}-\beta_1}{s_{\hat{\beta_1}}}$$

3、给出一个显著水平$$\alpha$$，自由度是n-k-1，查表可得临界值$$t_{\alpha/2}$$，如果$$t>t_{\alpha/2}$$，拒绝原假设$$H_0$$，$$\beta_i \ne 0$$。否则，接受原假设$$\beta_i=0$$。

### 多重共线性

现在有多个变量了，她们彼此之间应该无关的才好，但是她们在一些情况下，可能相关了，书上说，这种相关性是有“毒害”的，是会导致，会让回归方程出现问题，具体什么问题和什么原因，教材上没有说，我也没去深究，就当做结论吧。

既然不好，就要先发现它，然后再消除掉它。

#### 多重性判别

**方法1：做自变量间相关性检验**

就是对变量之间两两做相关性检验，并对这个相关系数，做显著性检验。

这个可以参考“一元回归中相关性分析”章节，计算出相关系数，这里有个细节，既然都算出来相关系数了，干嘛还要做显著性检验，原因是：

>接下来一个问题，$$r$$可不是总体的相关性$$\rho$$，它只是抽样的，那么一个问题是，这个抽样能代表总体么？也就是所谓r的可靠性、显著性。

**方法2：观察对系数检验的结果**

在做多元回归的线性回归F通过后，但是，某些系数检验（也就是某些系数为0了）没有通过，那些系数，可能和别的变量间存在共线性。要警觉了。

**方法3：容忍度（tolerance）、方差扩大因子（viariance inflation factor VIF）**

某个变量的“容忍度（tolerance）” 等于=：把这个变量变成“因变量”，把他变成y（之前的y不要了，暂时扔掉），然后，去算你新造出来这个“y”，和剩余的那些“x”们，他们的判定系数$$R_i^2$$（啥是判定系数$$R_i^2$$来着？数学就怕学了后面的忘了前面的，它是在一元回归的拟合度评价里，来判断，你造的这个回归公式，是不是很好地拟合了真实的y，不过，现在这个y，是你的那个被考察的变量而已）

我理解，就是逼着看看这个变量，假y，是不是可以被其他人（其他x），线性表达出来，如果能，那我还要它干嘛呢？对吧，我是这么理解的。所以，用1减去他，他就会更小（他要是能被别人拟合，他的$$R^2$$J就会很大，$$1-R^2$$就会很小）。

而方差扩大因子：$$VIF = \frac{1}{1-R_i^2}$$，如果共线的话，就会很大（取了倒数了嘛），一般认为大于10，也就是$$1-R^2<0.1$$，也就是$$R^2>0.9$$，就认为，这个变量和别人存在严重共现了。

#### 处理多重性

那共线了，咋办？

第一个方法，就是删，直接给丫删了不就得了，只要想关的都给丫删了。

第二个方法，没看懂！

说是，"对y的推测，要限定在自变量的样本的范围内"，啥意思？我理解是，为了防止共线导致的预测不准，你的x必须要在你过去生成模型的样本的定义域内。那超过了咋办？总之，没太理解，好吧，我将来还是粗暴的用第一种方法把，删删删！

书上，建议，如无必要，尽量用尽可能少的变量，尽量不要引入新变量，恩，牢记！

### 变量选择

上面也提到了，如无必要，尽量选择少的变量，那么，问题就是，如何选？

#### 逐步回归

这种选法，就是一个一个地来，每次增加1个变量，引入后，再查一遍已有的变量的t统计量显著不显著（显著性是啥来着？就是它该不该为0，$$\beta_i \ne 0$$），这里我有个好奇？为何这种显著性还会变？引入一个新的兄弟，你丫（你的显著性）就变了？


统计量为：$$t=\frac{\hat{\beta_i}-\beta_i}{s_{\hat{\beta_i}}},i>0$$，它符合自由度为n-k-1的t分布。

其中，$$s_{\hat{\beta}\_1}=\frac{s_e}{\sqrt{\sum x^2_i - \frac{1}{n}(\sum x_i)^2}}$$。

而
$$
s_{e}=\sqrt{\frac{\sum\left(y_{i}-\hat{y}\_{i}\right)^{2}}{n-2}}=\sqrt{\frac{S S E}{n-2}}=\sqrt{M S E}
$$

## 参考

- [《统计学》 贾俊平](https://book.douban.com/subject/10956491/)


